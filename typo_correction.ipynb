{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Typo correction #"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Preprocessing ##"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from typing import List\n",
    "from keras import Model\n",
    "from keras.callbacks import ReduceLROnPlateau, EarlyStopping, TensorBoard, ModelCheckpoint\n",
    "from keras.layers import Bidirectional\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "class CharacterTokenizer:\n",
    "    def __init__(self, max_word_len: int):\n",
    "        self._charset = list('abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ _-')\n",
    "        self._char_to_index_dict = {}\n",
    "        self._index_to_char_dict = {}\n",
    "        self._max_word_len = max_word_len\n",
    "        self._init_translate_dicts()\n",
    "\n",
    "    def _init_translate_dicts(self):\n",
    "        for index, char in enumerate(self._charset):\n",
    "            self._char_to_index_dict[char] = index\n",
    "            self._index_to_char_dict[index] = char\n",
    "\n",
    "    def get_charset_len(self) -> int:\n",
    "        return len(self._charset)\n",
    "\n",
    "    def encode_one_hot(self, word: str) -> List:\n",
    "        one_hot = np.zeros((self._max_word_len, len(self._charset)), dtype=np.float32)\n",
    "        for index, char in enumerate(word):\n",
    "            one_hot[index, self._char_to_index_dict[char]] = 1.0\n",
    "        return one_hot\n",
    "\n",
    "    def decode_one_hot_prediction(self, one_hot_matrix: List) -> str:\n",
    "        #cutoff = 1.0e-1\n",
    "        adjusted_one_hot = []\n",
    "        for vector in one_hot_matrix:\n",
    "            max_idx = np.argmax(vector)\n",
    "            adjusted_vector = np.zeros(len(vector))\n",
    "            #if vector[max_idx] > cutoff:\n",
    "            adjusted_vector[max_idx] = 1.0\n",
    "            adjusted_one_hot.append(adjusted_vector)\n",
    "        return self.decode_word(adjusted_one_hot)               \n",
    "\n",
    "    def decode_word(self, one_hot: List) -> str:\n",
    "        word = ''\n",
    "        for encoded_char in one_hot:\n",
    "            if max(encoded_char) != 0:\n",
    "                word += self._index_to_char_dict[np.where(encoded_char == 1.0)[0][0]]\n",
    "        return word"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "outputs": [],
   "source": [
    "from typing import List, Tuple\n",
    "import numpy as np\n",
    "from math import ceil\n",
    "\n",
    "def load_dataset(filename: str) -> Tuple:\n",
    "    x = []\n",
    "    y = []\n",
    "    with open(filename) as f:\n",
    "        content = f.read()\n",
    "    lines = content.strip().split('\\n')\n",
    "    for line in lines:\n",
    "        elements = line.split('\\t')\n",
    "        x.append(elements[0])\n",
    "        y.append(elements[1])\n",
    "    return x, y\n",
    "\n",
    "def add_padding_to_dataset(x: List, y: List, max_len:int) -> Tuple:\n",
    "    for i in range(0, len(x)):\n",
    "        for _ in range(0, max_len - len(x[i])):\n",
    "            x[i] += ' '\n",
    "        for _ in range(0, max_len - len(y[i])):\n",
    "            y[i] += ' '\n",
    "    return x, y\n",
    "\n",
    "def get_max_char_len(x, y) -> int:\n",
    "    x_max = len(max(x, key=len))\n",
    "    y_max = len(max(y, key=len))\n",
    "    return x_max if x_max > y_max else y_max\n",
    "\n",
    "def encode(x: List, y: List, tokenizer: CharacterTokenizer) -> Tuple:\n",
    "    for idx, _ in enumerate(x):\n",
    "        x[idx] = tokenizer.encode_one_hot(x[idx])\n",
    "        y[idx] = tokenizer.encode_one_hot(y[idx])\n",
    "    return x, y\n",
    "\n",
    "def split_data(x: List, y: List, ratio: float = 0.8) -> Tuple:\n",
    "    train_num = ceil(len(x) * ratio)\n",
    "    return (np.array(x[:train_num]),\n",
    "            np.array(y[:train_num]),\n",
    "            np.array(x[train_num:]),\n",
    "            np.array(y[train_num:]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "outputs": [],
   "source": [
    "from tensorflow.keras import Input, Model\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "def autoencoder(input_shape) -> Model:\n",
    "    _, x = input_shape\n",
    "    \n",
    "    input_layer = Input(shape=input_shape)\n",
    "    encoder_1 = Dense(x - 5, activation='relu')(input_layer)\n",
    "    encoded_2 = Dense(x - 15, activation='relu')(encoder_1)\n",
    "    encoded_3 = Dense(x - 20, activation='relu')(encoded_2)\n",
    "\n",
    "    bottleneck = Dense(1000, activation='relu')(encoded_3) \n",
    "\n",
    "    decoded_1 = Dense(x - 20, activation='relu')(bottleneck)\n",
    "    decoded_2 = Dense(x - 15, activation='relu')(decoded_1)\n",
    "    decoded_3 = Dense(x - 10, activation='relu')(decoded_2)\n",
    "\n",
    "    out = Dense(x, activation='sigmoid')(decoded_3)\n",
    "\n",
    "    model = Model(input_layer, out)\n",
    "    return model\n",
    "\n",
    "def autoencoder_(input_shape) -> Model:\n",
    "    _, x = input_shape\n",
    "    \n",
    "    input_layer = Input(input_shape)\n",
    "    \n",
    "    encoder = Dense(x - 5, activation='relu')(input_layer)\n",
    "    encoder = Dense(x - 15, activation='relu')(encoder)\n",
    "    encoder = Dense(x - 20, activation='relu')(encoder)\n",
    "\n",
    "    bottleneck = Dense(1000, activation='relu')(encoder) \n",
    "\n",
    "    decoder = Dense(x - 20, activation='relu')(bottleneck)\n",
    "    decoder = Dense(x - 15, activation='relu')(decoder)\n",
    "    decoder = Dense(x - 10, activation='relu')(decoder)\n",
    "    %\n",
    "    output_layer = Dense(x, activation='sigmoid')(decoder)\n",
    "    model = Model(input_layer, output_layer)\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training ##"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "outputs": [
    {
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__setattr__\u001b[1;34m(self, name, value)\u001b[0m\n\u001b[0;32m   2238\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2239\u001b[1;33m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtracking\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAutoTrackable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__setattr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2240\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: can't set attribute",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-92-4287f8bf2d01>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msplit_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mratio\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.9\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[0mae\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mautoencoder_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax_len\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_charset_len\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[0madam\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moptimizers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.001\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclipnorm\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-91-3d3ea6bfc8dd>\u001b[0m in \u001b[0;36mautoencoder_\u001b[1;34m(input_shape)\u001b[0m\n\u001b[0;32m     30\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSequential\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m     model.add(Bidirectional(LSTM(x, activation='relu', return_sequences=True, dropout=0.1),\n\u001b[1;32m---> 32\u001b[1;33m                             merge_mode='sum'))\n\u001b[0m\u001b[0;32m     33\u001b[0m     model.add(Bidirectional(LSTM(x, activation='relu', return_sequences=True,\n\u001b[0;32m     34\u001b[0m                                  dropout=0.1), merge_mode='sum'))\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\layers\\wrappers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, layer, merge_mode, weights, **kwargs)\u001b[0m\n\u001b[0;32m    366\u001b[0m                              \u001b[1;34m'Merge mode should be one of '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    367\u001b[0m                              '{\"sum\", \"mul\", \"ave\", \"concat\", None}')\n\u001b[1;32m--> 368\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_set_sublayers\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    369\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmerge_mode\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmerge_mode\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    370\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mweights\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     28\u001b[0m         \u001b[0mprev_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_DISABLE_TRACKING\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m         \u001b[0m_DISABLE_TRACKING\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m         \u001b[0m_DISABLE_TRACKING\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprev_value\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\layers\\wrappers.py\u001b[0m in \u001b[0;36m_set_sublayers\u001b[1;34m(self, layer)\u001b[0m\n\u001b[0;32m    390\u001b[0m         \u001b[0mconfig\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'go_backwards'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'go_backwards'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    391\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward_layer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_config\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 392\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward_layer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'forward_'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward_layer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    393\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward_layer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'backward_'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward_layer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    394\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__setattr__\u001b[1;34m(self, name, value)\u001b[0m\n\u001b[0;32m   2242\u001b[0m             ('Can\\'t set the attribute \"{}\", likely because it conflicts with '\n\u001b[0;32m   2243\u001b[0m              \u001b[1;34m'an existing read-only @property of the object. Please choose a '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2244\u001b[1;33m              'different name.').format(name))\n\u001b[0m\u001b[0;32m   2245\u001b[0m       \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2246\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: Can't set the attribute \"name\", likely because it conflicts with an existing read-only @property of the object. Please choose a different name."
     ],
     "ename": "AttributeError",
     "evalue": "Can't set the attribute \"name\", likely because it conflicts with an existing read-only @property of the object. Please choose a different name.",
     "output_type": "error"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import optimizers\n",
    "\n",
    "X, Y = load_dataset('typo-corpus-r1.txt')\n",
    "max_len = get_max_char_len(X, Y)\n",
    "tokenizer = CharacterTokenizer(max_word_len=max_len)\n",
    "X, Y = add_padding_to_dataset(X, Y, max_len)\n",
    "X, Y = encode(X, Y, tokenizer)\n",
    "X_train, Y_train, X_test, Y_test = split_data(X, Y, ratio=0.9)\n",
    "\n",
    "ae = autoencoder_(input_shape=(max_len, tokenizer.get_charset_len()))\n",
    "adam = optimizers.Adam(learning_rate=0.001, clipnorm=1)\n",
    "\n",
    "ae.compile(optimizer=adam, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "ae.summary()\n",
    "\n",
    "# callbacks\n",
    "log_dir = \"logs\\\\fit\\\\\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "ae.fit(X_train, Y_train, epochs=50, batch_size=128, verbose=0, \n",
    "       callbacks=[tensorboard_callback])#, shuffle=True)\n",
    "\n",
    "score = ae.evaluate(X_test, Y_test, verbose=0)\n",
    "print(f'## \\nTest set: \\nLoss: {score[0]}\\nAccuracy: {score[1]}')\n",
    "print('## \\nSample predictions:')\n",
    "\n",
    "predicted_y = ae.predict(X_test[:20])\n",
    "for i in range (0, 20):\n",
    "    print(f'Input: {tokenizer.decode_word(X_test[i])}')\n",
    "    print(f'Predicted: {tokenizer.decode_one_hot_prediction(predicted_y[i])}')\n",
    "    print(f'Real: {tokenizer.decode_word(Y_test[i])}\\n')\n",
    "\n",
    "ae.save('models/')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}